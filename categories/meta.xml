<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Pollard's Rho (Posts about meta)</title><link>https://nicholas-miklaucic.github.io/</link><description></description><atom:link href="https://nicholas-miklaucic.github.io/categories/meta.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2019 &lt;a href="mailto:nicholas.miklaucic@gmail.com"&gt;Nicholas Miklaucic&lt;/a&gt; </copyright><lastBuildDate>Sat, 03 Aug 2019 01:47:19 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Hello, World!</title><link>https://nicholas-miklaucic.github.io/posts/hello-world/</link><dc:creator>Nicholas Miklaucic</dc:creator><description>&lt;p&gt;
This is the first public page for this blog: a place where I will record thoughts and essays on
various topics, usually pretty close to the intersection of math, data, and computing. I'll try to
make things generally accessible, and I'll warn you in advance if something requires background
beyond high school math. (We'll see how well I do!)
&lt;/p&gt;

&lt;p&gt;
So, hello world!&lt;sup&gt;&lt;a id="fnr.1" class="footref" href="https://nicholas-miklaucic.github.io/posts/hello-world/#fn.1"&gt;1&lt;/a&gt;&lt;/sup&gt; I thought the first thing I should probably do is explain why I go by the tag
&lt;code&gt;PollardsRho&lt;/code&gt; in various places and why I decided to give this blog the title "Pollard's Rho." Simply
put, it's my favorite algorithm: kind of an esoteric category to have a favorite in, but I'm sure by
the end of this you'll see what I mean.
&lt;/p&gt;

&lt;div id="outline-container-org7c5ce79" class="outline-2"&gt;
&lt;h2 id="org7c5ce79"&gt;Problem&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7c5ce79"&gt;
&lt;p&gt;
I'll be talking about using Pollard's rho for integer factorization, probably the most famous and
important problem underlying modern cryptography. We have a big number \(n\), and we want to find its
prime factors. It's helpful to note that we really just need to find a single prime factor: we can
then divide \(n\) by that factor and restart the algorithm.
&lt;/p&gt;

&lt;p&gt;
Note that different factoring algorithms depend on different things about the input number \(n\). Some
do very well for certain kinds of \(n\) and not for others: unlike, say, sorting, where most
algorithms depend on some function solely of the input length, factoring algorithms all do different
things. We'll call \(p\) the smallest prime factor of \(n\) for our analysis of the runtime of Pollard's
rho. (This means \(p &amp;lt; \sqrt{n}\).)
&lt;/p&gt;

&lt;p&gt;
Our target to beat is simple trial division: guess and check. This has an asymptotic runtime of
\(O(\sqrt{n})\). (Big-O notation, as this is called, means essentially "behaves like the inner
expression as \(n\) increases." We only care about how it grows: \(200n\) or \(n + 100000\) are still
\(O(n)\), because doubling the input will at most double the output.) 
&lt;/p&gt;

&lt;p&gt;
However, because we usually care about how an algorithm varies as a function of the &lt;i&gt;length&lt;/i&gt; of an
input, this should really be \(O(\sqrt{2^b}) = O(2^{b/2})\), where \(b\) is the number of bits used to
represent \(n\). This makes trial division &lt;i&gt;exponential&lt;/i&gt;: increasing the number of bits by a constant
amount multiplicatively increases the time it takes to factor.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org384e991" class="outline-2"&gt;
&lt;h2 id="org384e991"&gt;Ideas&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org384e991"&gt;
&lt;p&gt;
I think it's helpful, when analyzing algorithms, to think about what is &lt;i&gt;incidental&lt;/i&gt; and what is
&lt;i&gt;germane&lt;/i&gt;. What aspects of an algorithm can be changed without significantly affecting its asymptotic
performance, and what aspects of an algorithm are essential? What insight or technique does this
algorithm use to do better than the simplest approach?
&lt;/p&gt;

&lt;p&gt;
Pollard's rho uses two important insights:
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5bc6ed6" class="outline-3"&gt;
&lt;h3 id="org5bc6ed6"&gt;Insight One: The Euclidean algorithm allows us to solve a related problem quickly.&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5bc6ed6"&gt;
&lt;p&gt;
Many algorithms in computer science rely on related, easier problems, and Pollard's rho is no
different. The &lt;a href="https://en.wikipedia.org/wiki/Euclidean_algorithm"&gt;Euclidean algorithm&lt;/a&gt;, invented thousands of years ago (!), solves the problem of
finding the greatest common divisor of two numbers. This GCD, if it's not \(1\) or either of the input
numbers, is a divisor of both, which is exactly what we want. Importantly, the Euclidean algorithm
runs in polynomial time, meaning that it's significantly faster than something like trial division
where the input size is in the top of an exponent. Here, adding another digit to both numbers
doesn't multiplicatively increase the running time.
&lt;/p&gt;

&lt;p&gt;
The problem for us is that simply computing the GCD of our input \(n\) and a bunch of other numbers
isn't really improving on trial division, because figuring out which numbers will give us a useful
result is as hard to factoring \(n\). 
&lt;/p&gt;

&lt;p&gt;
The right frame of mind for this problem is now to think about it like this: we're trying to find a
pair of numbers that are &lt;i&gt;equivalent&lt;/i&gt; in a certain way (specifically, modulo \(p\), so we can compute their
GCD and find \(p\)). This leads us to our second insight:
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge622354" class="outline-3"&gt;
&lt;h3 id="orge622354"&gt;Insight Two: The birthday paradox means that finding &lt;i&gt;any&lt;/i&gt; equivalent pair of something takes much fewer distinct things to do than finding a pair with some specific element.&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge622354"&gt;
&lt;p&gt;
The &lt;a href="https://en.wikipedia.org/wiki/Birthday_problem"&gt;birthday paradox&lt;/a&gt; is usually phrased like this: what's the smallest number of people such that
it's more likely than not at least two share a birthday? Counterintuitively, this number is far
smaller than the number of possibilities, 365: it's only 23! In general, the expected number grows
with the square root of the number of possibilities. The easiest intuitive explanation is that the
number of distinct combinations with \(n\) people is \(O(n^2)\), and only one of them needs to be
equal.
&lt;/p&gt;

&lt;p&gt;
Now, how does John Pollard use these two ideas?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9d55243" class="outline-2"&gt;
&lt;h2 id="org9d55243"&gt;The Algorithm&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9d55243"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Take your input number \(n\), and define a function \(f(x) = x^2 + 1 \pmod{n}\). The exact polynomial
isn't that important (one of those incidental details I mentioned earlier), but \(f(x)\) shouldn't
share factors with \(x\): this is the simplest way of doing that.&lt;/li&gt;
&lt;li&gt;Use this to make a sequence, starting at \(2\): \(2, f(2), f(f(2))\), and so on. Call this sequence
\(x_1, x_2, x_3\), etc.&lt;/li&gt;
&lt;li&gt;Use a cycle-finding algorithm to find some pair such that \(\gcd(|x_i - x_j|, n) \neq 1\). I call
this &lt;i&gt;cycle-finding&lt;/i&gt; for reasons that will be apparent shortly.&lt;/li&gt;
&lt;li&gt;Once you find such a pair, if the GCD is \(n\), then restart with different parameters. Otherwise,
you have found a factor of \(n\) and the algorithm is a success.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
Why does this work? What does it have to do with the birthday paradox? What is its runtime?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2d39992" class="outline-2"&gt;
&lt;h2 id="org2d39992"&gt;Analysis&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org2d39992"&gt;
&lt;p&gt;
The reason I call step 3 cycle-finding is because we can really think of this sequence \(x_1, x_2, \dots\)
as "shadowing" the real sequence we care about: \(x_1 \pmod{p}, x_2 \pmod{p}, \dots\)
&lt;/p&gt;

&lt;p&gt;
Obviously, because we don't know \(p\), we can't observe the second sequence (which I'll call \(y_1, y_2\)
, etc.) directly. However, this second sequence has to repeat eventually: there are finitely many
possible values. When it does, with arbitrary \(y_i = y_j\), we'll have that \(|x_i - x_j|\) is a multiple
of \(p\): thus, its GCD with \(n\) will probably&lt;sup&gt;&lt;a id="fnr.2" class="footref" href="https://nicholas-miklaucic.github.io/posts/hello-world/#fn.2"&gt;2&lt;/a&gt;&lt;/sup&gt; be \(p\), and we'll have achieved our goal.
&lt;/p&gt;

&lt;p&gt;
Note how this incorporates both of our insights. We use GCDs because they're fast to compute and
allow us to check whether a given number is a multiple of \(p\) without knowing its value. The second
insight makes this faster than just trying numbers in sequence: getting a value of \(0\) modulo \(p\),
which is what we'd need if we were just trying random GCDs, is much slower than finding any two
numbers with the same value modulo \(p\). Because there are \(p\) possible values, if the sequence \(y_1,
y_2, \dots\) is random-ish, which we'll assume&lt;sup&gt;&lt;a id="fnr.3" class="footref" href="https://nicholas-miklaucic.github.io/posts/hello-world/#fn.3"&gt;3&lt;/a&gt;&lt;/sup&gt;, by the birthday paradox we should expect a success with
with about \(O(\sqrt{p})\) different values.
&lt;/p&gt;

&lt;p&gt;
One detail which I'd put in the incidental category, but which I nonetheless need to mention, is the
existence of fast algorithms for finding a cycle in a sequence like this one. The standard&lt;sup&gt;&lt;a id="fnr.4" class="footref" href="https://nicholas-miklaucic.github.io/posts/hello-world/#fn.4"&gt;4&lt;/a&gt;&lt;/sup&gt; one I'll
mention is &lt;i&gt;Floyd's algorithm:&lt;/i&gt; keep track of two numbers, \(x_i\) and \(x_j\), and at every step increase
\(i\) by 1 and \(j\) by 2. Repeat until success. This is a huge improvement over keeping track of every
number you've seen, with only a small cost in terms of how efficiently it finds a cycle.
&lt;/p&gt;

&lt;p&gt;
This allows us to find a factor in expected time \(O(\sqrt{p})\), which as we mentioned earlier we can
approximate as \(O(\sqrt{\sqrt{n}}) = O(2^{b/4})\). (The Euclidean algorithm is faster, and so its
runtime doesn't play a role here.) This is a significant improvement over trial division. It's
especially important when \(n\) has a small prime factor \(p\), because the runtime only depends on the
smallest prime factor.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgfb8fcea" class="outline-2"&gt;
&lt;h2 id="orgfb8fcea"&gt;Why is this cool?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfb8fcea"&gt;
&lt;p&gt;
There are two reasons I really like this algorithm:
&lt;/p&gt;

&lt;p&gt;
Firstly, it has a cool name. It's called Pollard's rho algorithm (Ï is what rho looks like, for
reference) because of what it looks like if you draw the sequence repeating in a certain way. From
Wikimedia:
&lt;/p&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://upload.wikimedia.org/wikipedia/commons/4/47/Pollard_rho_cycle.jpg" alt="Pollard_rho_cycle.jpg"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
See the resemblance?
&lt;/p&gt;

&lt;p&gt;
Secondly, this algorithm was published in 1975.&lt;sup&gt;&lt;a id="fnr.5" class="footref" href="https://nicholas-miklaucic.github.io/posts/hello-world/#fn.5"&gt;5&lt;/a&gt;&lt;/sup&gt; Integer factorization became important in
cryptography from a practical perspective when the &lt;a href="https://doi.org/10.1145/359340.359342"&gt;RSA cryptosystem&lt;/a&gt; was published in 1978. Nowadays,
this problem underlies an enormous amount of modern encryption: every time you send your credit card
info over the Internet and that info stays safe, you should probably thank the fact that this
problem is hard.
&lt;/p&gt;

&lt;p&gt;
Given this, you'd think that useful algorithms for factoring integers would have not been that big
of a deal until 1978, especially given that computers weren't really that big of a deal then either
(or the Internet). But John Pollard, not a programmer but a mathematician, invented this anyway. I
think it's a wonderful testament to the virtues of exploration for its own sake: it's the equivalent
of climbing a mountain because it's there. I like to hope that a tool as strikingly simple and
elegant as Pollard's discovery doesn't often come from merely wanting to break RSA and steal
someone's money, or to someone working solely for a paycheck. Instead, I think that real insight
into hard problems often only comes from the drive that accompanies a sense of beauty and wonder at
the profound patterns that underlie mathematics, computing, data, and our world.
&lt;/p&gt;

&lt;p&gt;
It's an idea I aspire to, and keeping a reminder of Pollard's rho in my life helps me get there. I
hope it helps you too.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="footnotes"&gt;
&lt;h2 class="footnotes"&gt;Footnotes: &lt;/h2&gt;
&lt;div id="text-footnotes"&gt;

&lt;div class="footdef"&gt;&lt;sup&gt;&lt;a id="fn.1" class="footnum" href="https://nicholas-miklaucic.github.io/posts/hello-world/#fnr.1"&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;div class="footpara"&gt;&lt;p class="footpara"&gt;
A bit optimistic!
&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class="footdef"&gt;&lt;sup&gt;&lt;a id="fn.2" class="footnum" href="https://nicholas-miklaucic.github.io/posts/hello-world/#fnr.2"&gt;2&lt;/a&gt;&lt;/sup&gt; &lt;div class="footpara"&gt;&lt;p class="footpara"&gt;
We can occasionally get very unlucky, so the multiple of \(p\) that \(|x_i - x_j|\) is just so
happens to also be divisible by \(\frac{n}{p}\). This doesn't really impact the analysis because of
its rarity: we should just pick a different starting point and try again.
&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class="footdef"&gt;&lt;sup&gt;&lt;a id="fn.3" class="footnum" href="https://nicholas-miklaucic.github.io/posts/hello-world/#fnr.3"&gt;3&lt;/a&gt;&lt;/sup&gt; &lt;div class="footpara"&gt;&lt;p class="footpara"&gt;
This is a detail that significantly complicates a rigorous analysis, but it's not really
important for an intuitive understanding of why this might work.
&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class="footdef"&gt;&lt;sup&gt;&lt;a id="fn.4" class="footnum" href="https://nicholas-miklaucic.github.io/posts/hello-world/#fnr.4"&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;div class="footpara"&gt;&lt;p class="footpara"&gt;
This isn't the fastest algorithm, but it's space-efficient, easy to understand, and elegant.
&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class="footdef"&gt;&lt;sup&gt;&lt;a id="fn.5" class="footnum" href="https://nicholas-miklaucic.github.io/posts/hello-world/#fnr.5"&gt;5&lt;/a&gt;&lt;/sup&gt; &lt;div class="footpara"&gt;&lt;p class="footpara"&gt;
&lt;a href="https://doi.org/10.1007/BF01933667"&gt;https://doi.org/10.1007/BF01933667&lt;/a&gt;
&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;


&lt;/div&gt;
&lt;/div&gt;</description><category>algorithms</category><category>cs</category><category>meta</category><guid>https://nicholas-miklaucic.github.io/posts/hello-world/</guid><pubDate>Fri, 02 Aug 2019 23:35:07 GMT</pubDate></item></channel></rss>
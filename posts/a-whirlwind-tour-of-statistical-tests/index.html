<!DOCTYPE html>
<html prefix="
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Statistical tests explained in Python">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Pollard&#8217;s Rho · A Whirlwind Tour of Statistical Tests </title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="../../assets/demo.css" type="text/css">
<!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-164231105-1"></script><script>
     window.dataLayer = window.dataLayer || [];
     function gtag(){dataLayer.push(arguments);}
     gtag('js', new Date());

     gtag('config', 'UA-164231105-1');
    </script><meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://nicholas-miklaucic.github.io/posts/a-whirlwind-tour-of-statistical-tests/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Nicholas Miklaucic">
</head>
<body class="">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="hsidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://nicholas-miklaucic.github.io/">
                      <h1 id="brand"><a class="no-tufte-underline" href="https://nicholas-miklaucic.github.io/" title="Pollard's Rho" rel="home">

        <span id="blog-title">Pollard&#8217;s&nbsp;Rho</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead">Thoughts on math, computing, and&nbsp;data</p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="no-tufte-underline sidebar-nav-item" href="../../pages/about-me/">About Me</a>
        <a class="no-tufte-underline sidebar-nav-item" href="../../archive.html">Archive</a>
        <a class="no-tufte-underline sidebar-nav-item" href="../../categories/">Tags</a>
        <a class="no-tufte-underline sidebar-nav-item" href="../../rss.xml">RSS feed</a>
    
    
    </nav><footer id="footer"><span class="copyright">
              Contents © 2020         <a href="mailto:nicholas.miklaucic@gmail.com">Nicholas Miklaucic</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            </span>
            
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><h1 class="post-title p-name"><a href="." class="u-url">A Whirlwind Tour of Statistical&nbsp;Tests</a></h1>

    <span class="post-date">
      <time class="published dt-published" datetime="2019-11-07T12:08:21-05:00" itemprop="datePublished" title="2019-11-07 12:08">2019-11-07 12:08</time></span>
        <meta name="description" itemprop="description" content="Statistical tests explained in Python">
<div class="e-content entry-content" itemprop="articleBody text">
    <p>
Hello&nbsp;again! 
</p>

<p>
Let&#8217;s say you&#8217;ve been working on your dataviz skills, and you think you&#8217;ve found an interesting
pattern or correlation in a dataset. How can you tell that whatever you&#8217;re seeing is significant?
How can you distinguish between random variation and legitimate, meaningful results? As it happens,
the mathematical branch of statistics is almost entirely devoted to questions like these. I think
aspiring data scientists need not be full-fledged statisticians, but everyone should be familiar
with the basic statistical tests, when to apply them, and how to interpret their&nbsp;results.
</p>

<p>
Today we&#8217;ll be sojourning through the land of statistical tests in Python, showing how they might be
used and interpreted. The goal is not that you&#8217;ll be able to apply this all immediately when you
need it; instead, the goal is that you can recognize when statistical tests might be useful and be
able to find the ones you need. (In an age of the Internet, I find recognition to be much more
important than memorizing everything: if you know what to Google, you can figure out how to code it,
but you can&#8217;t Google something you can&#8217;t put into words or something you can&#8217;t&nbsp;recognize.)
</p>

<!-- TEASER_END -->

<p>
We&#8217;ll be using Python for this, and so the library we&#8217;ll apply for these tests is the excellent
<code>scipy.stats</code> module. It contains an enormous variety of statistical distributions and statistical
tests, all implemented using the same basic format so you don&#8217;t have to learn 20 different
syntaxes. It has excellent <a href="https://docs.scipy.org/doc/scipy/reference/stats.html">documentation</a> for how the parameters and stuff work, but it&#8217;s missing an
explanation of why you&#8217;d want to use what it offers: I&#8217;m seeking to supplement that information&nbsp;here. 
</p>

<p>
The landscape I&#8217;m surveying today—statistical tests to determine if results are significant—is a
broad one, so it&#8217;ll be useful to first lay a foundation by getting some terminology and sketching a
rough taxonomy of the tools we&#8217;ll be using&nbsp;today.
</p>

<div id="outline-container-orgb2dd645" class="outline-2">
<h2 id="orgb2dd645">What Is a Statistical&nbsp;Test?</h2>
<div class="outline-text-2" id="text-orgb2dd645">
<p>
A statistical test, in the way I&#8217;ll be using it today, means a way of testing a hypothesis. That
hypothesis can be almost anything: perhaps it&#8217;s that the mean of one set of numbers is higher than
another, or that the means the same, or that a set of numbers is normally distributed. Inherent in
the idea of a <i>statistical</i> test is that there&#8217;s some inherent lack of knowledge: we only have a
sample, there&#8217;s some sort of variability, randomness is at play, etc. We&#8217;re trying to make
inferences using a limited set of information: otherwise most of these would be pretty
boring. (&#8220;Am I taller than my friend from Korea&#8221; is a much easier question than &#8220;given the limited
sample of Americans and Koreans I know, which population is&nbsp;taller.&#8221;)
</p>
</div>
<div id="outline-container-org3b313c4" class="outline-3">
<h3 id="org3b313c4">What Do Tests&nbsp;Do?</h3>
<div class="outline-text-3" id="text-org3b313c4">
<p>
A very important thing to keep in mind is that statistical tests <b>don&#8217;t prove that hypotheses are
true.</b> That is, they can&#8217;t by their nature confirm one specific pattern given the infinite
possibilities inherent in any judgment like that. Instead, statistical tests <b>disprove false
hypotheses.</b> They can say that a result would probably never occur by chance, but they can&#8217;t tell you
why that result did happen. This is an oft-misunderstood fact about statistics. The key in making
inferences is to set up your hypothesis in a way such that you can give strong evidence for it by
disproving something else. For example, in our earlier example of testing population means, we can
say that it&#8217;s likely that Americans are taller than Koreans because, assuming that the populations
had the same height and I knew a random sample of both, the chance that I would observe whatever I
do is really&nbsp;small. 
</p>

<p>
This hypothesis—that whatever pattern I&#8217;m seeing is not due to the pattern I hypothesize but instead
due to random chance—is called the <i>null hypothesis.</i> It&#8217;s really what all of these statistical tests
are testing. I&#8217;ll make sure to keep it very clear what hypotheses match up with which&nbsp;tests.
</p>
</div>
</div>
<div id="outline-container-org94be7ab" class="outline-3">
<h3 id="org94be7ab">Assumptions</h3>
<div class="outline-text-3" id="text-org94be7ab">
<p>
This brings us to our next point: of course, there&#8217;s no way I&#8217;d actually know a random sample of
either of those populations. Almost any statistical test has one or more assumptions about its data,
and these are also often misunderstood or not applied&nbsp;correctly.
</p>

<p>
This means that we should really think of any errors we have in two ways: <i>known unknowns</i>, and
<i>unknown unknowns</i>. Just like in American foreign policy, it&#8217;s the unknown unknowns that you have to
watch out for. Known unknowns are the errors that are reported by our tests: the statistical errors
that result from getting lucky or unlucky. Unknown unknowns are the errors introduced by bad
methodology, untrue assumptions, or unaccounted-for statistical effects. (Elsewhere on this blog
I&#8217;ve talked about regression to the mean: this is an excellent example of an effect that can make
tests that seem worthwhile actually completely&nbsp;useless.)
</p>

<p>
Of course, we don&#8217;t live in a perfect mathematical world and some error is inevitable. However, keep
in mind that the further we stretch our assumptions the less useful our tests are, and <i>our tests</i>
<i>won&#8217;t be able to report that uncertainty to us</i>. Every significance level or test statistic is
conditioned on our applying statistics properly, our data coming from a good source, etc. I stress
this because of just how easy it is to blindly trust numbers that come out of the computers to 10
decimal places. Keep in mind that what we get out is only as good as what we put in, and Python
isn&#8217;t magic: it can&#8217;t tell us that our data is bad or that our hypothesis is misleading or
incorrect. What that out of the way, how do we interpret our results assuming that our inputs are&nbsp;good?
</p>
</div>
</div>
<div id="outline-container-org01aa003" class="outline-3">
<h3 id="org01aa003">Interpretation</h3>
<div class="outline-text-3" id="text-org01aa003">
<p>
Tests will usually give a <i>test statistic</i>, some calculation based on our data, and that test
statistic will usually follow a <i>test distribution.</i> Given those two things, we can determine how
unlikely such a test statistic is given the assumptions of the null hypothesis. This, in turn, gives
us a <i>p-value</i>: the likelihood of observing the results we did assuming the conditions of the null
hypothesis. This is another commonly misunderstood but very important concept in statistics. This is
<b>definitely not</b> the chance we&#8217;re right. It&#8217;s, at best, the chance we aren&#8217;t completely wrong: the
chance that anything we&#8217;re seeing is entirely due to random variation. In the sciences, a value of
\(0.05\) is generally considered <i>significant</i>, although this changes a lot depending on context. (One
way of interpreting this is that, if you pick \(0.05\) as a cutoff, one out of every twenty tests you
run will be wrong and you won&#8217;t know. This means that running 500 tests will produce some
significant results purely by&nbsp;chance!)
</p>

<p>
OK, the philosophizing is over. Let&#8217;s get to the test&nbsp;themselves!
</p>
</div>
</div>
</div>
<div id="outline-container-org88df455" class="outline-2">
<h2 id="org88df455">The Landscape of Statistical&nbsp;Hypotheses</h2>
<div class="outline-text-2" id="text-org88df455">
<span class="marginnote"><p>
</p>
<p>
Obviously there are a lot of statistical tests out there: I couldn&#8217;t possibly summarize all of them
here. However, these are the ones I&#8217;ve seen the most often in scientific publications and my own
work, and my hope is that they&#8217;ll be the ones you run into the most as well. These are also an
excellent foundation for more complicated tests that you might run into in the future, like ANOVA
and other tests that work on multiple populations in a more complicated&nbsp;way.
</p>
</span>

<p>
There&#8217;s a couple of very useful kinds of questions we&#8217;re going to ask: these pop up all the time in
science and statistics, and so it&#8217;s reasonable to assume that most patterns you see can somehow be
corroborated by using one of these tests. We&#8217;ll break these up into a couple of&nbsp;categories.
</p>
<ul class="org-ul">
<li>I have one set of data and I want to test if it follows some&nbsp;distribution.</li>
<li>I have one set of data and I want to say something about its&nbsp;mean.</li>
<li>I have two sets of data in the same units. I want to compare their&nbsp;means.</li>
<li>I have two sets of data in pairs. I want to say something about how they&#8217;re&nbsp;related.</li>
</ul>
</div>

<div id="outline-container-orgc506853" class="outline-3">
<h3 id="orgc506853">Testing the Distribution of a Single&nbsp;Dataset</h3>
<div class="outline-text-3" id="text-orgc506853">
</div>
<div id="outline-container-orgd747393" class="outline-4">
<h4 id="orgd747393">Normality</h4>
<div class="outline-text-4" id="text-orgd747393">
<span class="marginnote"><p>

</p>
<p>
From now on, assume we&#8217;ve imported the Python libraries <code>numpy</code>, <code>pandas</code>, <code>seaborn</code>, and <code>scipy.stats</code> as
<code>np</code>, <code>pd</code>, <code>sns</code>, and <code>stats</code> respectively.
</p>
</span>

<p>
The reason I&#8217;m doing this one first is because it tends to be a prerequisite for other tests: a lot
of tests assume normality, and you should always check your&nbsp;assumptions.
</p>

<p>
We&#8217;ll start with the basics. Let&#8217;s say we have a dataset of 1000 <a href="https://en.wikipedia.org/wiki/PSAT/NMSQT?oldformat=true">PSAT</a> scores. Let&#8217;s show a couple
numbers from that dataset and make a&nbsp;histogram:
</p>

<div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">psats</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>  <span class="c1"># just to see a few numbers</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="mf">1010.</span><span class="p">,</span>  <span class="mf">770.</span><span class="p">,</span> <span class="mf">1340.</span><span class="p">,</span> <span class="mf">1070.</span><span class="p">,</span> <span class="mf">1310.</span><span class="p">])</span>
</pre></div>

<p>
<img src="../../images/stat-tests/psat-hist.png" alt="nil"></p>

<p>
I think this data seems like it follows a normal distribution. As it happens, a lot of statistical
tests require that, so let&#8217;s see how good that assumption is with our first test. As it happens,
there are several normality tests, but the one we&#8217;ll use and the one I&#8217;d generally recommend is the
<i>Shapiro-Wilk test</i>: 
</p>

<div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">shapiro</span><span class="p">(</span><span class="n">psats</span><span class="p">)</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="p">(</span><span class="mf">0.9979366660118103</span><span class="p">,</span> <span class="mf">0.2571161985397339</span><span class="p">)</span>
</pre></div>

<p>
This outputs a combination of test statistic first and the p-value second. I haven&#8217;t said what our
null hypothesis is yet, so interpreting this isn&#8217;t possible. In this case, our null hypothesis is
that the data <i>was</i> drawn from a normal distribution. Note that a p-value above 0.05, as in this
case, doesn&#8217;t mean we can positively prove it <i>is</i> normal, but rather that we haven&#8217;t proved it
isn&#8217;t: again, mind the double negative because it&#8217;s very important. In this case, we can interpret
this as saying that it&#8217;s reasonable to assume that our data is normal or normal-ish. A graphical
comparison makes this seem pretty&nbsp;logical:
</p>

<p>
<img src="../../images/stat-tests/psat-fit.png" alt="nil"></p>

<p>
It isn&#8217;t perfect, but with 1000 numbers it isn&#8217;t expected to be. Let&#8217;s say we now have 10,000
points. What does the Shapiro-Wilk test say&nbsp;now?
</p>

<p>
<img src="../../images/stat-tests/psat-fit-more.png" alt="nil"></p>

<div class="highlight"><pre><span></span><span class="c1"># now with more data</span>
<span class="n">stats</span><span class="o">.</span><span class="n">shapiro</span><span class="p">(</span><span class="n">psats</span><span class="p">)</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="ne">UserWarning</span><span class="p">:</span> <span class="n">p</span><span class="o">-</span><span class="n">value</span> <span class="n">may</span> <span class="ow">not</span> <span class="n">be</span> <span class="n">accurate</span> <span class="k">for</span> <span class="n">N</span> <span class="o">&gt;</span> <span class="mf">5000.</span>

<span class="p">(</span><span class="mf">0.9985979795455933</span><span class="p">,</span> <span class="mf">9.862207406285961e-08</span><span class="p">)</span>
</pre></div>

<p>
We can see that the histogram has some weird spikes and our test now returns an incredibly low
p-value: our PSAT test data is definitely not normally distributed. When I was trying to convince
you the reader that normality <i>was</i> a valid assumption to make, in my series on Bayesian statistical
fallacies, I used a CDF plot (plotting how many values were below a given threshold versus how many
would be expected to under the assumption of normality) to make my point. What does that plot show&nbsp;here?
</p>

<p>
<img src="../../images/stat-tests/psat-cdf.png" alt="nil"></p>

<p>
Those are actually two different curves: it&#8217;s nearly perfect! What&#8217;s the issue&nbsp;here?
</p>

<p>
<code>scipy.stats</code> is smart, and it tried to warn us about this. Essentially, the issue is that, given
enough data, even the tiniest difference is significant. Our data is definitely not 100% normally
distributed: it has a minimum and maximum, and it can only take values that are multiples
of 10. Most of the time, when we ask if something is normal, we really mean if it&#8217;s close
enough—if the difference is small. You can see that the test statistic is actually very similar to
last time. This is an indication that the difference is small in absolute terms, but that given
10,000 data points it should still never happen. The upshot of this&nbsp;is:
</p>

<ul class="org-ul">
<li>Use the Shapiro-Wilk test to determine if a dataset is reasonably close to a normal&nbsp;distribution.</li>
<li>For small populations, use the p-value to determine if your assumption is&nbsp;valid.</li>
<li>For larger populations (I&#8217;d say around 1000 numbers or more), use the test statistic instead:
the p-value gives misleading&nbsp;results.</li>
</ul>
<p>
For the record: the data I used was generated by sampling from a normal distribution, rounding to
the nearest 10, and ensuring that all of the points were between 320 and 1520. For most purposes
I&#8217;d consider that close enough to a normal distribution to model that way: your mileage may&nbsp;vary.
</p>

<p>
I promise most of these aren&#8217;t going to be as&nbsp;tricky!
</p>
</div>
</div>

<div id="outline-container-org1361334" class="outline-4">
<h4 id="org1361334">Other&nbsp;Distributions</h4>
<div class="outline-text-4" id="text-org1361334">
<span class="marginnote"><p>

</p>
<p>
<span class="dquo">&#8220;</span>But Nicholas&#8221;, I hear you ask, &#8220;why couldn&#8217;t we just use this test for testing normality?&#8221; You
could: it&#8217;s just that the Shapiro-Wilk test is more precise and will detect smaller
differences. We&#8217;ve already covered that the test&#8217;s precision is more of a curse than a blessing,
though, so it&#8217;s not really that big of a&nbsp;difference.
</p>

</span>

<p>
If you want to test a different distribution, you can use the <i>Kolmogorov-Smirnov test</i>, a very
general and useful tool. It has the same caveat as above (it can detect tiny differences with large
enough sample sizes), but since we just made a big point about that I&#8217;ll gloss over it this&nbsp;time. 
</p>

<span class="marginnote"><p>
</p>
<p>
<code>scipy.stats</code> distributions have all sorts of useful functions: for example, I used the normal one to
make the expected CDF I used in the plot above. Check the docs to learn&nbsp;more.
</p>
</span>

<p>
Let&#8217;s say we have a bunch of data between 0 and 1 and we want to know if it&#8217;s <i>uniformly</i>
<i>distributed.</i> (Sorry, I couldn&#8217;t think of a cool real-world example for this one.) <code>scipy.stats</code> has a
ton of statistical distributions we can use (the normal distribution is <code>norm</code>, by the ways), and as
it happens it has <code>uniform</code> premade for&nbsp;us. 
</p>

<p>
The way the Kolmogorov-Smirnov test works is by essentially making the CDF plot I showed above, and
then finding the largest vertical difference and comparing that to what would be expected by
chance. The reason I&#8217;m mentioning this is because this test works well for any combination of
statistical distributions and datasets you want to test against. <code>scipy.stats</code> will happily estimate
your CDF from your data if you only have a sample, but it will use the distributions to be more&nbsp;exact.
</p>

<p>
Here&#8217;s a histogram of our data (1000 numbers) and the output of our test. The function is pretty
smart: we can pass in any CDF we want, or we can pass in the name of a distribution. (To use a
second set of numbers, use <code>stats.ks_2samp</code>.) It&#8217;ll automatically fit the distribution if there&#8217;s
anything to fit, or we can pass in the arguments we need as a tuple. (The inputs are <code>loc</code> and <code>scale</code>,
to represent a uniform distribution between <code>loc</code> and <code>loc + scale</code>. Here it doesn&#8217;t actually matter
whether we add them, because <code>scipy</code> will fit this distribution&nbsp;anyway.)
</p>

<p>
<img src="../../images/stat-tests/uniform-hist.png" alt="nil"></p>

<div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">kstest</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">'uniform'</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="n">KstestResult</span><span class="p">(</span><span class="n">statistic</span><span class="o">=</span><span class="mf">0.025222960578</span><span class="p">,</span> <span class="n">pvalue</span><span class="o">=</span><span class="mf">0.54801947328</span><span class="p">)</span>
</pre></div>

<p>
The null hypothesis is that the data was drawn from this distribution: again, there&#8217;s an inherent
double negative. Here the p-value is quite high, which makes sense: this data was actually generated
by sampling from a uniform distribution, so a result like this is very likely to occur by random&nbsp;chance. 
</p>
</div>
</div>
<div id="outline-container-org9594049" class="outline-4">
<h4 id="org9594049">Discrete&nbsp;Distributions</h4>
<div class="outline-text-4" id="text-org9594049">
<p>
What if our distribution isn&#8217;t continuous? For this, we can use the high-school stats classic: the
<i>Pearson&#8217;s chi-squared test.</i> Calculating this by hand isn&#8217;t actually that hard, but we didn&#8217;t learn
to use Python so we could write things out like chumps! The chi-squared test (or \(\chi^2\) if you
want to be fancy) assumes that your observing something with a fixed list of possibilities, and you
have some collected counts for those observations and some expected counts given a distribution. We
want to test against the <i>null hypothesis</i> that our data was generated from this distribution. There&#8217;s
one assumption we should check: the frequencies we&#8217;re looking for can&#8217;t be too small. The rule of
thumb is that your observed and expected frequencies should be over 5 in each&nbsp;category.
</p>

<p>
This is very common for catching cheating or funny business! Let&#8217;s say you think a die is loaded. We
can imagine rolling it 100 times (no, I&#8217;m not actually doing this, that&#8217;s what computers are
for). Let&#8217;s plot the roll frequencies we&nbsp;get:
</p>

<p>
<img src="../../images/stat-tests/die-rolls.png" alt="nil"></p>

<p>
There&#8217;s some variation, but that&#8217;s to be expected: ask any board game player and they&#8217;ll tell you
probability doesn&#8217;t always even out. The question is: can we show that this wouldn&#8217;t happen by&nbsp;chance?
</p>

<div class="highlight"><pre><span></span><span class="c1"># counts is the data I, um, "collected"</span>
<span class="nb">print</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span> <span class="o">/</span> <span class="mi">6</span><span class="p">]</span> <span class="o">*</span> <span class="mi">6</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">stat</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">chisquare</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Stat: '</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">stat</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'p: '</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="p">[</span><span class="mi">34</span> <span class="mi">23</span> <span class="mi">13</span>  <span class="mi">7</span>  <span class="mi">8</span> <span class="mi">15</span><span class="p">]</span>
<span class="p">[</span><span class="mf">16.7</span> <span class="mf">16.7</span> <span class="mf">16.7</span> <span class="mf">16.7</span> <span class="mf">16.7</span> <span class="mf">16.7</span><span class="p">]</span>
<span class="n">Stat</span><span class="p">:</span>  <span class="mf">31.52</span>
<span class="n">p</span><span class="p">:</span>  <span class="n">nan</span>
</pre></div>

<p>
Our p-value was so low <code>scipy</code> gave up! This happens a lot with this test: it&#8217;s very precise and gives
very large values for small differences. Here, we can be almost certain that this die is
loaded. (We&#8217;re correct in this case: I simulated it so 1 had a 40% chance and 2 had a 20%
chance, giving the rest 10%&nbsp;chances.)
</p>
</div>
</div>
</div>
<div id="outline-container-org40574f9" class="outline-3">
<h3 id="org40574f9">Testing a Threshold For a Single&nbsp;Dataset</h3>
<div class="outline-text-3" id="text-org40574f9">
<p>
This problem is another classic statistics question. Let&#8217;s say I&#8217;m arguing with my friend and they
claim that soccer games are lame because, on average, there&#8217;s only two goals a game. Let&#8217;s test this
claim against some recent Champions League games as of this&nbsp;writing:
</p>

<span class="marginnote"><p>
</p>
<p>
Scores from&nbsp;ESPN.
</p>
</span>

<div class="highlight"><pre><span></span><span class="c1"># from November 5-6, 2019</span>
<span class="n">total_scores</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> 
    <span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
    <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
    <span class="mi">5</span>
<span class="p">]</span>
</pre></div>

<p>
The test we&#8217;re going to use is called the <i>Student&#8217;s t-test</i>, named for William Sealy Gosset who wrote
under the pseudonym Student. It&#8217;s pretty simple: it tests the null hypothesis that the mean of the
whole population we&#8217;re sampling is equal to the given&nbsp;value.
</p>

<div class="highlight"><pre><span></span><span class="n">stat</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">ttest_1samp</span><span class="p">(</span><span class="n">total_scores</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"p ="</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.012</span>
</pre></div>

<p>
This is indicative that our friend is wrong: the data doesn&#8217;t support it. Note that this is
<i>two-sided</i>: it doesn&#8217;t claim whether the true mean is larger or smaller. It&#8217;s a good rule to use
two-sided tests in most cases: it&#8217;s not usually fair to assume the mean is above 2 or below&nbsp;it.
</p>
</div>
</div>

<div id="outline-container-orgc9d6701" class="outline-3">
<h3 id="orgc9d6701">Comparing Means From Two&nbsp;Datasets</h3>
<div class="outline-text-3" id="text-orgc9d6701">
<p>
This test can be extended to work for two distributions. There&#8217;s actually two distinct ways this
often pops up that we should distinguish: <i>paired</i> and <i>unpaired</i>. In <i>paired</i> tests, the two datasets are
the same size and correspond with each other. An example of this would be testing whether a tutor
significantly improved the SAT scores of their tutees. In the <i>unpaired</i> kind, we have different
populations (perhaps of different sizes) and we want to compare their means. An example of this
would be trying to determine if Champions League games have more goals than Premier League
games. This is pretty simple to do in Python and has a familiar&nbsp;form:
</p>

<div class="highlight"><pre><span></span><span class="c1"># "ind" for "independent" meaning "unpaired"</span>
<span class="n">stats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span><span class="n">champions</span><span class="p">,</span> <span class="n">premier</span><span class="p">)</span>
<span class="c1"># "rel" for "related" meaning "paired"</span>
<span class="n">stats</span><span class="o">.</span><span class="n">ttest_rel</span><span class="p">(</span><span class="n">before</span><span class="p">,</span> <span class="n">after</span><span class="p">)</span>
</pre></div>

<p>
There are some assumptions here. We&#8217;re assuming that the data have the same variance (regardless of
whether we know it), and that they&#8217;re distributed in the same way. This generally holds up well in
real life, and it isn&#8217;t that far off if you bend the rules a little, but it&#8217;s worth&nbsp;remembering.
</p>
</div>
</div>
<div id="outline-container-org3dd07ff" class="outline-3">
<h3 id="org3dd07ff">Relational&nbsp;Statistics</h3>
<div class="outline-text-3" id="text-org3dd07ff">
<p>
Let&#8217;s say we have some paired data in different units and we want to figure out if they&#8217;re
related. An example of this would be asking whether SAT scores correlate with freshman GPA in
college. The example we&#8217;ll use is gas mileage vs. weight for some car&nbsp;models:
</p>

<span class="marginnote"><p>
</p>
<p>
This dataset comes with <code>seaborn</code>: <code>sns.load_dataset("mpg")</code> is all you&nbsp;need.
</p>
</span>

<p>
<img src="../../images/stat-tests/mpg-vs-weight.png" alt="nil"></p>

<p>
I&#8217;ve drawn in the regression line. Note that data can have all sorts of relationships, and it&#8217;s a
very difficult question to answer how two sets of numbers relate in general. We&#8217;re going to limit
ourselves to a single feasible version of this problem: do higher values of miles per gallon
correlate with lower values of weight, as they seem to&nbsp;here?
</p>

<p>
We can use several tests for this: the most common is <i>Pearson&#8217;s correlation coefficient</i>, or
\(r\). This assumes that the \(x\) and \(y\) datasets are normally distributed, but fudging this a little
doesn&#8217;t usually have too many problems, at least when you combine it with a graphical test like the
one above. Let&#8217;s see what comes&nbsp;out:
</p>

<div class="highlight"><pre><span></span><span class="c1"># assuming we've loaded our data into df</span>
<span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">'mpg'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">'weight'</span><span class="p">])</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="p">(</span><span class="o">-</span><span class="mf">0.831740933244335</span><span class="p">,</span> <span class="mf">2.9727995640500577e-103</span><span class="p">)</span>
</pre></div>

<p>
The test statistic here is quite common and easy to interpret on its own: -1 means perfect
negative correlation (the data is perfectly linear with a negative slope), 0 means no correlation
whatsoever, and 1 means a perfect positive correlation (the data is perfectly linear with a
positive slope.) This is often squared to get the common \(R^2\) statistic, which is usually
interpreted as the percentage of the variance in the y-values that can be explained by the&nbsp;x-values.
</p>

<p>
How extreme is this \(r\) value? Well, the p-value has to be some sort of record! The chance that this
would happen by chance if miles per gallon and weight really had no correlation (the null
hypothesis) is astronomically low, which makes sense when we see the&nbsp;data.
</p>

<p>
I should really stress that this is best used in <i>combination</i> with visuals. You can get really
nonsensical results&nbsp;otherwise:
</p>

<div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>

<div class="highlight"><pre><span></span><span class="p">(</span><span class="mf">0.8165214368885029</span><span class="p">,</span> <span class="mf">0.002164602347197218</span><span class="p">)</span>
</pre></div>

<p>
So there&#8217;s a relationship, right? Well, maybe&nbsp;not:
</p>

<p>
<img src="../../images/stat-tests/anscombe-iv.png" alt="nil"></p>
</div>
</div>
</div>

<div id="outline-container-orgf27d055" class="outline-2">
<h2 id="orgf27d055">Wrapping&nbsp;Up</h2>
<div class="outline-text-2" id="text-orgf27d055">
<p>
This provides a nice segue into my final point: it&#8217;s easy to overuse statistical tests or apply them
in situations where their results are meaningless. These tests are best used to confirm whether a
result you already hypothesize is attributable to&nbsp;chance. 
</p>


<span class="marginnote"><p>
</p>
<p>
This is a large part of why the morning show &#8220;coffee cures cancer&#8221; lines are so misleading. If you
track what 500 people eat for a year, and then test every possible correlation of food with disease
incidence, you&#8217;ll almost certainly find a couple significant relationships by random chance. Then,
if you publish those results as if they were your hypothesis, you have a significant&nbsp;result! 
</p>

<p>
The solution to this is <i>replication</i>: use tests to confirm something, not to figure out what to
think. That part&#8217;s something you have to do first, or you can&#8217;t use the results you&nbsp;get.
</p>
</span>

<p>
Being careful to verify rather than hypothesize with testing also avoids the problem of <i>p-hacking</i>:
essentially, using lots of statistical tests and then claiming you had as your hypothesis any of the
ones that happen to come out significant. This essentially abuses the fact that people treat
p-values as some sort of divine revelation of significance rather than as a probabliity. This is an
easy trap when the actual calculations are so&nbsp;easy! 
</p>

<p>
So, to&nbsp;conclude:
</p>

<ul class="org-ul">
<li>Shapiro-Wilk tests normality, but you probably don&#8217;t really mean &#8220;is my data perfectly normal&#8221; so
take it with a grain of&nbsp;salt.</li>
<li>Kolmogorov-Smirnov tests any distribution, with the same caveat as above: the real world is not
all exact statistical&nbsp;distributions.</li>
<li>Chi-squared tests are used for discrete distributions and checking if results are too good to be&nbsp;true.</li>
<li>T-tests are used for reasoning about the mean of a&nbsp;sample.</li>
<li>Pearson&#8217;s correlation can be used to reason about the linear relationship of two variables, but
you should be looking at your data&nbsp;first!</li>
<li>Don&#8217;t test to figure out what you want to claim: claim something, and then test&nbsp;it.</li>
</ul>
<p>
Hope this helps any of you get through your stats classes or check if a pattern you find is
significant. If you&#8217;ve liked this, you can see the other posts on this blog to see Python data
science in action or to learn about visualizing your data in&nbsp;Python.
</p>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><p itemprop="keywords" class="tags">
            <span class="tag"><a class="p-category" href="../../categories/guide/" rel="tag">guide</a></span>
            <span class="tag"><a class="p-category" href="../../categories/math/" rel="tag">math</a></span>
      </p>

    
    </nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="nicholas-miklaucic-github-io",
            disqus_url="https://nicholas-miklaucic.github.io/posts/a-whirlwind-tour-of-statistical-tests/",
        disqus_title="A Whirlwind Tour of Statistical Tests",
        disqus_identifier="cache/posts/a-whirlwind-tour-of-statistical-tests-for-programmers.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
        </script></article><script>var disqus_shortname="nicholas-miklaucic-github-io";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
            <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script type="text/javascript">
    var MTIProjectId='a0c6a264-432e-4405-ab1a-8b212bb5c7c7';
    (function() {
    var mtiTracking = document.createElement('script');
    mtiTracking.type='text/javascript';
    mtiTracking.async='true';
    mtiTracking.src='mtiFontTrackingCode.js';
    (document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild( mtiTracking );
    })();
    </script>
</body>
</html>